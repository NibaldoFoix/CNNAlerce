{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Primer_clasificador.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "07uL-bKEoGJa",
        "eFr9MfnEoLar",
        "RB35_8f3oP_G",
        "j0SdhB1gosHB",
        "RlFOOlnQtQSE",
        "lOvNqWnCSpuD",
        "0Re-QW53ggtj",
        "1PcLY4s-sH6D",
        "V_VF-zUsPC81",
        "magMlreJ3JyI",
        "rfWQmaBL50X4"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07uL-bKEoGJa"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvVSJuN5w-ha"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lZ-WozXqnkZ"
      },
      "source": [
        "import pandas as pd\n",
        "import math \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import warnings\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    warnings.warn('GPU device not found')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFr9MfnEoLar"
      },
      "source": [
        "## Selección de sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EvmEl7Lqnkg"
      },
      "source": [
        "#leemos los datos (nibaldo)\n",
        "train_set=pd.read_pickle('/gdrive/My Drive/Colab Notebooks/CNNAlerce/train_set.pkl', compression='infer')\n",
        "val_set=pd.read_pickle('/gdrive/My Drive/Colab Notebooks/CNNAlerce/val_set.pkl', compression='infer')\n",
        "test_set=pd.read_pickle('/gdrive/My Drive/Colab Notebooks/CNNAlerce/test_set.pkl', compression='infer')\n",
        "\n",
        "\n",
        "'''\n",
        "#para diego:\n",
        "train_set=pd.read_pickle('/gdrive/My Drive/CNNAlerce/train_set.pkl', compression='infer')\n",
        "val_set=pd.read_pickle('/gdrive/My Drive/CNNAlerce/val_set.pkl', compression='infer')\n",
        "test_set=pd.read_pickle('/gdrive/My Drive/CNNAlerce/test_set.pkl', compression='infer')\n",
        "'''\n",
        "\n",
        "#Construcción de sets:\n",
        "x_train = np.array(train_set['im_21']) #['images'] para 63 pixeles\n",
        "x_val = np.array(val_set['im_21']) #['images'] para 63 pixeles\n",
        "x_test = np.array(test_set['im_21']) #['images'] para 63 pixeles\n",
        "y_train = np.array(train_set['labels'])\n",
        "y_val = np.array(val_set['labels'])\n",
        "y_test = np.array(test_set['labels'])\n",
        "del train_set\n",
        "del val_set\n",
        "del test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB35_8f3oP_G"
      },
      "source": [
        "##Modelos sin Invarianza Rotacional\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj8e6mSJoVik"
      },
      "source": [
        "### Modelo preliminar \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeHx1yiaqnki"
      },
      "source": [
        "#Clasificador preliminar\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "#Parte convolucional\n",
        "model.add(tf.keras.layers.Conv2D(16, kernel_size=(3, 3), input_shape=(21, 21, 3), activation='relu'))#, padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))#, padding='same'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))#, padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))#, padding='same'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "#Fully-connected\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer, loss, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhuEKW7B0za-"
      },
      "source": [
        "#Entrenamiento:\n",
        "summary = model.fit(x=x_train, y=y_train, batch_size=1460, epochs=20, verbose=1, validation_data=(x_val, y_val), shuffle=True, callbacks=[tf.keras.callbacks.TensorBoard('./logs/preliminar', update_freq=100)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co7Dw5Aj00T-"
      },
      "source": [
        "#Testing\n",
        "model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0SdhB1gosHB"
      },
      "source": [
        "### Modelo paper sin rotaciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Omh3QaZoucN"
      },
      "source": [
        "#Clasificador paper\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "#model.add(tf.keras.layers.ZeroPadding2D(padding=((3,3),(3,3)), data_format=None))\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(4, 4), input_shape=(21, 21, 3), padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer, loss, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC2pbkDqIm7V"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kcXZCKnIspX"
      },
      "source": [
        "#Entrenamiento:\n",
        "nombre_ensayo = './logs/sroverfitting_3'\n",
        "summary = model.fit(x=x_train, y=y_train, batch_size=1460, epochs=19, verbose=1, validation_data=(x_val, y_val), shuffle=True, callbacks=[tf.keras.callbacks.TensorBoard(nombre_ensayo, update_freq=100)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSc0MBmpIxTd"
      },
      "source": [
        "#Testing\n",
        "model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQqvWN-0sZhT"
      },
      "source": [
        "## Modelo con Invarianza Rotacional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb7e2-0H0OoA"
      },
      "source": [
        "### Construcción Red"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjyy8P3zAAer"
      },
      "source": [
        "img_inputs = tf.keras.Input(shape=(21, 21, 3))\n",
        "rot_90 = tf.keras.layers.Lambda(lambda x: tf.image.rot90(x, k=1))(img_inputs)\n",
        "rot_180 = tf.keras.layers.Lambda(lambda x: tf.image.rot90(x, k=2))(img_inputs)\n",
        "rot_270 = tf.keras.layers.Lambda(lambda x: tf.image.rot90(x, k=3))(img_inputs)\n",
        "\n",
        "#RED COMPARTIDA\n",
        "conv_1 = tf.keras.layers.Conv2D(32, kernel_size=(4, 4), padding='same', activation='relu')#(img_inputs)\n",
        "conv_2 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu')#(conv_1)\n",
        "max_pool_1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))#(conv_2)\n",
        "conv_3 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu')#(max_pool_1)\n",
        "conv_4 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu')#(conv_3)\n",
        "conv_5 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu')#(conv_4)\n",
        "max_pool_2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))#(conv_5)\n",
        "\n",
        "#\n",
        "out1 = conv_1(img_inputs)\n",
        "out1 = conv_2(out1)\n",
        "out1 = max_pool_1(out1)\n",
        "out1 = conv_3(out1)\n",
        "out1 = conv_4(out1)\n",
        "out1 = conv_5(out1)\n",
        "out1 = max_pool_2(out1)\n",
        "#\n",
        "out2 = conv_1(rot_90)\n",
        "out2 = conv_2(out2)\n",
        "out2 = max_pool_1(out2)\n",
        "out2 = conv_3(out2)\n",
        "out2 = conv_4(out2)\n",
        "out2 = conv_5(out2)\n",
        "out2 = max_pool_2(out2)\n",
        "#\n",
        "out3 = conv_1(rot_180)\n",
        "out3 = conv_2(out3)\n",
        "out3 = max_pool_1(out3)\n",
        "out3 = conv_3(out3)\n",
        "out3 = conv_4(out3)\n",
        "out3 = conv_5(out3)\n",
        "out3 = max_pool_2(out3)\n",
        "#\n",
        "out4 = conv_1(rot_270)\n",
        "out4 = conv_2(out4)\n",
        "out4 = max_pool_1(out4)\n",
        "out4 = conv_3(out4)\n",
        "out4 = conv_4(out4)\n",
        "out4 = conv_5(out4)\n",
        "out4 = max_pool_2(out4)\n",
        "\n",
        "concatenate = tf.stack([out1, out2, out3, out4])\n",
        "mean_pool = tf.reduce_mean(concatenate, axis=0)\n",
        "flatten = tf.keras.layers.Flatten()(mean_pool)\n",
        "dense_1 = tf.keras.layers.Dense(64, activation='relu')(flatten)\n",
        "dense_2 = tf.keras.layers.Dense(64, activation='relu')(dense_1)\n",
        "outputs = tf.keras.layers.Dense(5, activation='softmax')(dense_2)\n",
        "\n",
        "model = tf.keras.Model(inputs=img_inputs, outputs=outputs, name='funcional')\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPtwYMxpCDAv"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E4MWq8pz4Ag"
      },
      "source": [
        "### Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEWMZ-Yhzspv"
      },
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "model.compile(optimizer, loss, metrics=['accuracy'])\r\n",
        "nombre_ensayo = './logs/ce26_4'\r\n",
        "summary = model.fit(x=x_train, y=y_train, batch_size=1460, epochs=22, verbose=1, validation_data=(x_val, y_val), shuffle=True, callbacks=[tf.keras.callbacks.TensorBoard(nombre_ensayo, update_freq=100)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-zl4G5F7Wrl"
      },
      "source": [
        "#Testing\r\n",
        "model.evaluate(x=x_test, y=y_test, batch_size=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UWZN2TOPNme"
      },
      "source": [
        "### Entropia Regulada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URCarG08nh2Y"
      },
      "source": [
        "#### Construcción Función"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYpm4Ok-M1um"
      },
      "source": [
        "\"\"\"Implement Regulated Cross Entropy\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "#import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow.python.keras.losses import LossFunctionWrapper\n",
        "#from tensorflow_addons.utils.keras_utils import LossFunctionWrapper\n",
        "from tensorflow_addons.utils.types import FloatTensorLike, TensorLike\n",
        "from typeguard import typechecked\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class CrossEntropyRegulated(LossFunctionWrapper):\n",
        "    @typechecked\n",
        "    def __init__(\n",
        "        self,\n",
        "        from_logits: bool = False,\n",
        "        beta: FloatTensorLike = 0.5,\n",
        "        name: str = \"crossentropy_regulated\",\n",
        "    ):\n",
        "        super().__init__(\n",
        "            crossentropy_regulated,\n",
        "            name=name,\n",
        "            from_logits=from_logits,\n",
        "            beta=beta)\n",
        "\n",
        "@tf.function\n",
        "def crossentropy_regulated(\n",
        "    y_true: TensorLike,\n",
        "    y_pred: TensorLike,\n",
        "    beta: FloatTensorLike = 0.5,\n",
        "    from_logits: bool = False,\n",
        ") -> tf.Tensor:\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    y_true = tf.convert_to_tensor(y_true, dtype=y_pred.dtype)\n",
        "\n",
        "    '''\n",
        "    print('y_pred: '+str(y_pred.shape))\n",
        "    print(y_pred[0][0])\n",
        "    print('y_pred: '+str(y_true.shape))\n",
        "    print(y_true[0][0])\n",
        "    '''\n",
        "\n",
        "    # Get the cross_entropy for each entry\n",
        "    ce = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n",
        "    \n",
        "    \n",
        "    #se cambia formaro para que calzen las dimensiones\n",
        "    #y_pred2=ops.convert_to_tensor_v2_with_dispatch(y_pred)\n",
        "    y_pred2 = math_ops.cast(y_true, y_true.dtype)\n",
        "\n",
        "    #ce2 = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n",
        "    ce2=K.sparse_categorical_crossentropy(y_pred2, y_pred, from_logits=from_logits)\n",
        "    \n",
        "\n",
        "    # compute the final loss and return\n",
        "    return tf.reduce_mean(ce)+beta*tf.reduce_mean(ce2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrs4txxe3sd-"
      },
      "source": [
        "#Para esta funcion de costos se necesita pasar a float. \n",
        "y_val=y_val.astype(np.float32)\n",
        "y_train=y_train.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG2YxtTt-Gr9"
      },
      "source": [
        "#se compila con la nueva funcion de costos\n",
        "#f1 = tfa.losses.SigmoidFocalCrossEntropy()\n",
        "f1=CrossEntropyRegulated()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeU_BIFunouQ"
      },
      "source": [
        "#### Compilación y entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnxWE3y1MeAi"
      },
      "source": [
        "model.compile(optimizer, loss=f1, metrics=['accuracy'])\r\n",
        "nombre_ensayo = './logs/er26_2'\r\n",
        "summary = model.fit(x=x_train, y=y_train, batch_size=1460, epochs=26, verbose=1, validation_data=(x_val, y_val), shuffle=True, callbacks=[tf.keras.callbacks.TensorBoard(nombre_ensayo, update_freq=100)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJwOlzlaKV9t"
      },
      "source": [
        "model.evaluate(x=x_test.astype(np.float32), y=y_test.astype(np.float32), batch_size=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfWQmaBL50X4"
      },
      "source": [
        "## Métricas y Matriz de Confusión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGLtrjqRXjcu"
      },
      "source": [
        "#Testing\n",
        "model.evaluate(x=x_test.astype(np.float32), y=y_test.astype(np.float32), batch_size=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOpxHyIK7Cei"
      },
      "source": [
        "#Matriz de confusión\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import seaborn as sn\r\n",
        "\r\n",
        "y_test=y_test.astype(np.float32)\r\n",
        "x_test=x_test.astype(np.float32)\r\n",
        "\r\n",
        "prediction = model.predict(x_test, batch_size=32, verbose=1)  \r\n",
        "y_predicted = np.argmax(prediction, axis=1)  \r\n",
        "\r\n",
        "cnf_matrix = confusion_matrix(y_test, y_predicted)/200\r\n",
        "scnn_df_cm = pd.DataFrame(cnf_matrix, range(5), range(5))\r\n",
        "plt.figure(figsize = (10,8))  \r\n",
        "sn.set(font_scale=1.4) #for label size  \r\n",
        "sn.heatmap(scnn_df_cm, annot=True, cmap=\"Blues\", annot_kws={\"size\": 12}) # font size \r\n",
        "plt.title('Matriz de confusión normalizada') \r\n",
        "plt.ylabel('True label')\r\n",
        "plt.xlabel('Predicted label')\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYVGxX_GleNN"
      },
      "source": [
        "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
        "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "TP = np.diag(cnf_matrix)\n",
        "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "FP = FP.astype(float)\n",
        "FN = FN.astype(float)\n",
        "TP = TP.astype(float)\n",
        "TN = TN.astype(float)\n",
        "# Sensitivity, hit rate, recall, or true positive rate\n",
        "TPR = TP/(TP+FN)\n",
        "# Specificity or true negative rate\n",
        "TNR = TN/(TN+FP) \n",
        "# Precision or positive predictive value\n",
        "PPV = TP/(TP+FP)\n",
        "# Negative predictive value\n",
        "NPV = TN/(TN+FN)\n",
        "# Fall out or false positive rate\n",
        "FPR = FP/(FP+TN)\n",
        "# False negative rate\n",
        "FNR = FN/(TP+FN)\n",
        "# False discovery rate\n",
        "FDR = FP/(TP+FP)\n",
        "# Overall accuracy for each class\n",
        "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
        "\n",
        "accuracy = 100.0 * (TP + TN) / (TP + TN + FP + FN)\n",
        "precision = 100.0 * TP / (TP + FP)\n",
        "recall = 100.0 * TP / (TP + FN)\n",
        "\n",
        "print('accuracy :'+str(accuracy[1]))\n",
        "print('precision: '+str(precision[1]))\n",
        "print('recall: '+str(recall[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip5Gc1QfWHjG"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZsEAhBS2Sgn"
      },
      "source": [
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujCHd0kjWIsb"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}